<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction on COMP52315 – Performance Engineering</title><link>https://teaching.wence.uk/comp52315/</link><description>Recent content in Introduction on COMP52315 – Performance Engineering</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://teaching.wence.uk/comp52315/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction</title><link>https://teaching.wence.uk/comp52315/notes/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/notes/introduction/</guid><description>Resources in stored program computers # To understand the performance of a code, we need to have an understanding of what hardware resources it uses, and what resources the hardware provides.
All modern general purpose hardware uses a von Neumann architecture. That is, there is a memory which stores both the program code to be executed and the data for the program. This is attached to a processor (CPU) which contains execution units for executing individual instructions in the program code along with further parts of logical control and load/store of data.</description></item><item><title>Sum reductions</title><link>https://teaching.wence.uk/comp52315/exercises/exercise01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/exercises/exercise01/</guid><description>Benchmarking with likwid-bench # Overview # We&amp;rsquo;re going to look at the throughput of a very simple piece of code
float reduce(int N, const double *restrict a) { float c = 0; for (int i = 0; i &amp;lt; N; i++) c += a[i]; return c; } when all of the data live in L1 cache.
We&amp;rsquo;ll do so on an AVX-capable core (where the single-precision vector width is 8).</description></item><item><title>Caches</title><link>https://teaching.wence.uk/comp52315/exercises/exercise02/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/exercises/exercise02/</guid><description>Measuring memory bandwidth in the memory hierarchy # The goal is to determine the memory bandwidth as a function of how much data we are moving on the Hamilton cores.
Again, as for the first exercise we will do this with likwid-bench. This time, however, we will use three different benchmarks
clcopy: Double-precision cache line copy, only touches first element of each cache line. clload: Double-precision cache line load, only loads first element of each cache line.</description></item><item><title>The memory hierarchy</title><link>https://teaching.wence.uk/comp52315/notes/memory/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/notes/memory/</guid><description>An overview of memory hierarchies # Reduction benchmark # In exercise 1 you looked at the performance of a vectorised and non-vectorised version of a very simple loop computing the sum of an array of floating point numbers.
In doing so, you produced a plot of the performance (in terms of floating point throughput) as a function of array size. You should have observed something similar to that shown here.</description></item><item><title>Memory bandwidth</title><link>https://teaching.wence.uk/comp52315/exercises/exercise03/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/exercises/exercise03/</guid><description>Measuring multi-core memory bandwidth # The goal of this exercise is to measure the memory bandwidth for various vector sizes as a function of the number of cores used to process the vector.
Again, we will do this with likwid-bench. This time, we will use the clload benchmark.
Setup # As usual, you should do this on a compute node on Hamilton. See the quickstart in exercise 1 if you can&amp;rsquo;t remember how to do this.</description></item><item><title>Performance models: roofline</title><link>https://teaching.wence.uk/comp52315/notes/roofline/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/notes/roofline/</guid><description>Models of performance # If our goal is to improve the performance of some code we should take a scientific approach. We must first define what we mean by performance. So far, we&amp;rsquo;ve talked about floating point throughput (GFlops/s) or memory bandwidth (GBytes/s). However, these are really secondary characteristics to the primary metric of performance of a code:
How long do I have to wait until I get the answer?</description></item><item><title>Measurement and profiling</title><link>https://teaching.wence.uk/comp52315/notes/measurements/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/notes/measurements/</guid><description>Performance measurements # So far, we&amp;rsquo;ve seen the roofline model, and observed that for floating point code, it allows us to get a high-level view of what coarse step we should be taking to improve the performance of our algorithm.
Now suppose that we do a roofline analysis for our code, we observe that it should be limited by floating point throughput, but we are nowhere near the roof.</description></item><item><title>Roofline analysis</title><link>https://teaching.wence.uk/comp52315/exercises/exercise04/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/exercises/exercise04/</guid><description>A roofline analysis of matrix-vector multiplication # The goal of this exercise is to perform a roofline analysis of matrix-vector multiplication. We will look at the effect that compiler flags have on the performance of generated code. Next, we will see if the performance that we obtain is independent of the problem size. Finally, we will investigate loop blocking.
Background # I provide an implementation1 in code/exercise04/dmvm.c, in C, of double-precision matrix-vector multiplication, which computes</description></item><item><title>Slides</title><link>https://teaching.wence.uk/comp52315/slides/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/slides/</guid><description>Slides 2021/22 edition # Slides for the live lectures. These will be augmented with annotated versions, links to recordings, and some short commentary after the fact. If you think you should have access to the recordings but don&amp;rsquo;t, please get in touch.
The long form notes add words in between the bullet points.
Session 1, annotated, video.
We introduced some ideas of computer architecture and talked about the motivation for the course.</description></item><item><title>Cache blocking/tiling</title><link>https://teaching.wence.uk/comp52315/notes/tiling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/notes/tiling/</guid><description>Achieving reasonable performance for loopy code # Many of the algorithms we encounter in scientific computing have quite &amp;ldquo;simple&amp;rdquo; data access patterns. Numerical code often has multiple nested loops with regular array indexing. This is actually a reason the roofline model is so successful: its optimistic assumptions are not too optimistic.
Despite this simplicity, on hardware with memory caches, we still need to do some work to turn this &amp;ldquo;simple&amp;rdquo; code into something that runs with reasonable performance.</description></item><item><title>Further resources</title><link>https://teaching.wence.uk/comp52315/resources/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/resources/</guid><description>Other reading/resources # Travis Downs writes an interesting, and tremendously detailed, low-level performance optimisation and measurement blog. John Regehr writes about compilers often including details of optimisation. For an interesting look at how far you can go down the rabbithole of optimising a small piece of code, see this sequence of posts on improving the performance of sorting small arrays by vectorisation (among other things). Denis Bakhvalov writes a blog on performance optimisation, with a focus on the &amp;ldquo;top-down&amp;rdquo; methodology.</description></item><item><title>Models and measurements</title><link>https://teaching.wence.uk/comp52315/exercises/exercise05/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/exercises/exercise05/</guid><description>Verifying a model with measurements # The goal of this exercise is to verify our model for the number of loads and stores in a stream benchmark using performance counters, accessed via likwid-perfctr.
Background # I provide an implementation (in code/exercise05/stream.c) written in C of the STREAM TRIAD benchmark. It provides scalar, SSE, and AVX implementations of the loop
double *a, *b, *c; ... for (i = 0; i &amp;amp;lt; N; i++) { c[i] = c[i] + a[i] * b[i]; } We will measure the number of loads and stores for this loop using likwid-perfctr.</description></item><item><title>Profiling</title><link>https://teaching.wence.uk/comp52315/exercises/exercise06/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/exercises/exercise06/</guid><description>Finding a hotspot and determining the execution limits # So far, we&amp;rsquo;ve only run very simple benchmarks. Now we&amp;rsquo;re going to try and find some information in a larger code. We will look at the miniMD application which has been developed as part of the Mantevo project. This is a molecular dynamics code that implements algorithms and data structures from a large research code, but in a small package that is amenable to benchmarking and trying out different optimisations.</description></item><item><title>Loop tiling matrix transpose</title><link>https://teaching.wence.uk/comp52315/exercises/exercise07/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/exercises/exercise07/</guid><description>The effect of loop tiling on matrix transpose # In lectures, we saw a model for throughput of a matrix transpose operation. Here we&amp;rsquo;re going to look at the effect on throughput of loop tiling. I provide an implementation of matrix transpose with and without one level of loop tiling.
As usual, these live in the repository. Compile the code # We&amp;rsquo;ll use the intel compiler to build this code.</description></item><item><title>Loop tiling matrix-matrix multiplication</title><link>https://teaching.wence.uk/comp52315/exercises/exercise08/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/exercises/exercise08/</guid><description>Simple loop tiling for matrix-matrix multiplication # Having looked at the effect of loop tiling schemes for increasing the throughput of matrix transpose operations in exercise 7, we&amp;rsquo;re now going to look at throughput of the loop-tiling scheme presented in lectures for matrix-matrix multiplication. I provide an implementation of matrix-matrix multiplication in code/exercise08/gemm.c that provides three different variants. A naive triple loop, a tiled version of the triple loop, and a tiled version that manually packs local buffers.</description></item><item><title>Compiler feedback and the BLIS DGEMM</title><link>https://teaching.wence.uk/comp52315/exercises/exercise09/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/exercises/exercise09/</guid><description>Getting compilers to the right thing # We&amp;rsquo;re going to look at how to convince the compiler to vectorise a loop the way we want it to.
As our starting point we&amp;rsquo;ll use a C version of the GEMM micro-kernel used in the BLIS framework.
Rather than doing this on Hamilton, since we&amp;rsquo;re not actually going to run the code, we will use the Compiler explorer which is an online frontend to trying out lots of different compilers.</description></item><item><title>Acknowledgements</title><link>https://teaching.wence.uk/comp52315/acknowledgements/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/acknowledgements/</guid><description>Acknowledgements # Some of the exercises adapt material from the HPC group at FAU. Their likwid tool is invaluable.</description></item><item><title>Stencil layer conditions</title><link>https://teaching.wence.uk/comp52315/exercises/exercise10/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/exercises/exercise10/</guid><description>Loop tiling for stencil codes # We&amp;rsquo;re going to investigate the layer condition loop tiling guidance for stencil codes. We&amp;rsquo;ll use as an exemplar the five-point finite difference stencil for the Laplacian on a regular grid in code/exercise10/fivepoint.c.
The layer condition model was developed by a group at Erlangen, and they provide an interactive calculator.
As usual, we&amp;rsquo;re going to use the Intel compiler on Hamilton, so after logging in and downloading the code, load the relevant modules</description></item><item><title>Contact details</title><link>https://teaching.wence.uk/comp52315/setup/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/setup/contact/</guid><description>Lecturer contact details # The course is taught by Lawrence Mitchell, the best way to get in touch is via email lawrence.mitchell@durham.ac.uk.
The course materials are also hosted on GitHub, so if you find mistakes in them please let me know there.</description></item><item><title>Hamilton accounts</title><link>https://teaching.wence.uk/comp52315/setup/hamilton/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/setup/hamilton/</guid><description>Access to Hamilton # For many of the exercises in the course, we will be using the Hamilton supercomputer. You should have already obtained an account on Hamilton as part of Core Ib. If you do not yet have one, please register following their instructions.
Since we&amp;rsquo;ll be logging in a lot, I also provide some tips on how to configure ssh for swifter login.
Quickstart reminder # If you want to remind yourself of how Hamilton works, take a look at the quickstart guide from PHYS52105.</description></item><item><title>ssh configuration</title><link>https://teaching.wence.uk/comp52315/setup/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/setup/configuration/</guid><description>ssh tips &amp;amp; tricks # Setting up simpler logins # It can be tedious to remember to type long login commands every time when logging in via ssh to hamilton. I therefore recommend that you set up an ssh config file.
Additionally, you might also want to set up ssh keys for passwordless login.
The ssh-config configuration file # Mac/Linux When you run ssh it reads a configuration file at $HOME/.</description></item><item><title>Unix resources</title><link>https://teaching.wence.uk/comp52315/setup/unix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/comp52315/setup/unix/</guid><description>Using Unix-like systems for computational science # This course presupposes some level of familiarity with commandline interfaces. You have encountered these already in the previous term. In case you need a quick refresher, I recommend the material produced by the Software Carpentry project.
They have a number of useful lessons and materials providing introductory training on how to do things like use the Unix shell, version control with git, and some introductory programming and plotting in Python.</description></item></channel></rss>